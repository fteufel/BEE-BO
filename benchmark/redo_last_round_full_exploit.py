'''
From an existing experiment, take the second-to-last round and run it again with full exploitation.
'''
import torch
import numpy as np
import json
import os
import argparse
import pandas as pd
from botorch.utils.transforms import standardize, normalize, unnormalize
from botorch.optim.optimize import optimize_acqf
from botorch.optim.initializers import gen_batch_initial_conditions
from botorch.models.gp_regression import SingleTaskGP
from gpytorch.mlls import ExactMarginalLogLikelihood
import gpytorch
from botorch.fit import fit_gpytorch_mll
from botorch.generation import gen_candidates_torch
from botorch import test_functions
from tqdm.auto import tqdm
import time
import random

from beebo import BatchedEnergyEntropyBO
from problems import EmbeddedHartmann
torch.set_default_dtype(torch.float64)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")



# https://github.com/AnHosu/opt-test-functions
def get_test_problem(config):


    if config['test_function'] == 'hartmann':
        test_fn = test_functions.Hartmann(dim=config['dim'], negate=True)
    elif config['test_function'] == 'styblinskitang':
        test_fn = test_functions.StyblinskiTang(dim=config['dim'], negate=True)
    elif config['test_function'] == 'rosenbrock':
        test_fn = test_functions.Rosenbrock(dim=config['dim'], negate=True)
    elif config['test_function'] == 'rastrigin':
        test_fn = test_functions.Rastrigin(dim=config['dim'], negate=True)
    elif config['test_function'] == 'ackley':
        test_fn = test_functions.Ackley(dim=config['dim'], negate=True)
    elif config['test_function'] == 'levy':
        test_fn = test_functions.Levy(dim=config['dim'], negate=True)
    elif config['test_function'] == 'shekel':
        test_fn = test_functions.Shekel(negate=True)
        if config['dim'] != 4:
            raise NotImplementedError('Shekel only implemented for dim=4')
    elif config['test_function'] == 'cosine':
        test_fn = test_functions.Cosine8() # this has 1 global maximum by default. No need to negate.
        if config['dim'] != 8:
            raise NotImplementedError('Cosine only implemented for dim=8')
    elif config['test_function'] == 'embeddedhartmann':
        test_fn = EmbeddedHartmann(dim=config['dim'], dim_hartmann=6, negate=True)
    elif config['test_function'] == 'powell':
        test_fn = test_functions.Powell(dim=config['dim'], negate=True)
    else:
        raise NotImplementedError(config['test_function'])

    return test_fn


def get_starting_points(n_points: int, bounds: torch.Tensor, seed: int = 123, optima: torch.Tensor = None, min_dist: float = 0.2):

    generator = torch.Generator().manual_seed(seed)

    dim = bounds.shape[1]
    point_counter = 0

    optima_unit_cube = normalize(optima, bounds) if optima is not None else None
    optima_unit_cube = optima_unit_cube.to(torch.get_default_dtype())

    all_train_x_raw = torch.zeros(n_points, dim).to(torch.get_default_dtype())
    while point_counter < n_points:
        train_x_raw = torch.rand(n_points, dim, generator=generator).to(torch.get_default_dtype())

        # reject points that don't have min_dist to optima (euklidean distance)
        if optima is not None:

            # TODO this is broken - not sure why.
            dist = torch.cdist(optima_unit_cube, train_x_raw)
            mask = dist.min(dim=0).values > min_dist
            train_x_raw = train_x_raw[mask]

            train_x_raw = train_x_raw[:n_points-point_counter]

        # unnormalize
        # TODO this is wrong.
        train_x_raw = unnormalize(train_x_raw, bounds)
        # for i in range(dim):
        #     train_x_raw[:,i] = (bounds[0][i]-bounds[1][i]) * train_x_raw[:,i] + bounds[1][i]

        all_train_x_raw[point_counter:point_counter+train_x_raw.shape[0]] = train_x_raw

        point_counter += train_x_raw.shape[0]

    # import ipdb; ipdb.set_trace()

    return all_train_x_raw


def get_acquisition_function(acq_fn, model, config, kernel_amplitude=None):
    if config['acq_fn'] == 'beebo':
        acq = BatchedEnergyEntropyBO(model, temperature=config['explore_parameter'], kernel_amplitude=kernel_amplitude, logdet_method=config['logdet_method'])
    elif config['acq_fn'] == 'qucb':
        from botorch.sampling import SobolQMCNormalSampler
        from botorch.acquisition import qUpperConfidenceBound
        sampler = SobolQMCNormalSampler(1024)
        # NOTE we **2 the explore_param because we want to control the internal beta.sqrt()
        acq = qUpperConfidenceBound(model, config['explore_parameter']**2, sampler)
    elif config['acq_fn'] == 'qei':
        from botorch.sampling import SobolQMCNormalSampler
        from botorch.acquisition import qLogExpectedImprovement
        sampler = SobolQMCNormalSampler(1024)
        best_f = model.train_targets.max().item()
        acq = qLogExpectedImprovement(model, best_f, sampler)
    elif config['acq_fn'] == 'ucb':
        from botorch.acquisition import UpperConfidenceBound
        acq = UpperConfidenceBound(model, config['explore_parameter'])
    else:
        raise NotImplementedError(acq_fn)

    return acq

def thompson_initial_conditions_generator(n, q, seed, d, model, n_candidates=10000):
    """Generate initial conditions for acquisition function optimization
    using Thompson sampling.
    
    Parameters
    ----------
        n: int
            The number of initial conditions to generate.
        q: int
            The batch size.
        seed: int
            The random seed.
        model: gpytorch.models.ExactGP
            The GP model.

    Returns
    -------
        torch.Tensor
            The initial conditions (n x q x d)
    """
    # print('Generating initial conditions using Thompson sampling')
    from botorch.generation import MaxPosteriorSampling
    from torch.quasirandom import SobolEngine

    if seed is None:
        seed = 123

    with gpytorch.settings.max_cholesky_size(float("inf")):
        with torch.no_grad():
            thompson_sampling = MaxPosteriorSampling(model=model, replacement=False)

            X_q = []
            for i in range(n):
                X_cand = SobolEngine(d, scramble=True, seed=seed+i).draw(n_candidates)
                X_q.append(thompson_sampling(X_cand.to(device), num_samples=q)) # (q x d)

            X_q = torch.stack(X_q) # (n_candidates x q x d)

    # print(f'Got starting points from Thompson sampling. Shape: {X_q.shape} ')
    return X_q.cpu()


def get_candidates(acq, config, bounds, initial_conditions=None):
    """Generate candidates using the acquisition function.

    Parameters
    ----------
        acq: botorch.acqusition.acquisition.AcquisitionFunction
            The acquisition function.
        config: dict
            Configuration dictionary.
        bounds: torch.Tensor 
            The bounds of the search space.
        initial_conditions: torch.Tensor 
            The initial conditions. Defaults to None. If this is provided,
            the raw_samples argument of optimize_acqf is ignored.
        use_thompson_sampling: bool
            Whether to use Thompson sampling to generate initial conditions.

    Returns
    -------
        points: torch.Tensor
            The generated candidates.
    """



    # this is necessary for the root decomposition in the MC sampler to work as expected
    with gpytorch.settings.max_root_decomposition_size(max(100, config['batch_size'])):#, gpytorch.settings.fast_pred_var(True):



        gen_candidates = gen_candidates_torch if config['opt']=='torch' else None
        points, value = optimize_acqf(
            acq, 
            q=config['batch_size'], 
            num_restarts=config['num_restarts'], 
            bounds=normalize(bounds, bounds), 
            raw_samples=config['raw_samples'] if initial_conditions is None else None, 
            gen_candidates= gen_candidates,
            batch_initial_conditions=initial_conditions,
            generator = None,
            # seed=config['seed']
            )
            

        return points.cpu(), value
    
    

def get_random_candidates(config, bounds):
    '''
    Generate random candidates.

    Parameters
    ----------
        config: dict
            Configuration dictionary.
        bounds: torch.Tensor
            The bounds of the search space.

    Returns
    -------
        points: torch.Tensor
            The generated candidates.
    '''
    dim = bounds.shape[1]
    points = torch.rand(config['batch_size'], dim)

    return points


def get_q1_plus_random_candidates(acq, config, bounds, initial_conditions=None):
    '''
    Do single-point BO and fill the rest of the batch with random points.

    Parameters
    ----------
        acq: botorch.acqusition.acquisition.AcquisitionFunction
            The acquisition function.
        config: dict
            Configuration dictionary.
        bounds: torch.Tensor
            The bounds of the search space.
        initial_conditions: torch.Tensor
            The initial conditions. Defaults to None. If this is provided,
            the raw_samples argument of optimize_acqf is ignored.

    Returns
    -------
        points: torch.Tensor
            The generated candidates.
    '''

    gen_candidates = gen_candidates_torch if config['opt']=='torch' else None
    points, value = optimize_acqf(
        acq, 
        q=1, 
        num_restarts=config['num_restarts'], 
        bounds=normalize(bounds, bounds), 
        raw_samples=config['raw_samples'] if initial_conditions is None else None, 
        gen_candidates= gen_candidates)
    
    random_points = get_random_candidates(config, bounds)

    # replace the first random point with the candidate.
    random_points[0] = points[0]

    return random_points, value


def get_matern_kernel_with_gamma_prior(
    ard_num_dims: int, batch_shape = None
):
    r"""Constructs the Scale-Matern kernel that is used by default by
    several models. This uses a Gamma(3.0, 6.0) prior for the lengthscale
    and a Gamma(2.0, 0.15) prior for the output scale.

    Adapted from BoTorch source to use KeOps instead.
    """
    return gpytorch.kernels.ScaleKernel(
        base_kernel=gpytorch.kernels.keops.MaternKernel(
            nu=2.5,
            ard_num_dims=ard_num_dims,
            batch_shape=batch_shape,
            lengthscale_prior=gpytorch.priors.torch_priors.GammaPrior(3.0, 6.0),
        ),
        batch_shape=batch_shape,
        outputscale_prior=gpytorch.priors.torch_priors.GammaPrior(2.0, 0.15),
    )


def run_one_round(test_problem, train_x: torch.Tensor, train_y: torch.Tensor, config):

    train_x = normalize(train_x, test_problem.bounds).to(device)
    train_y = standardize(train_y).to(device)


    print('Setting up GP')
    if config['keops']:
        kernel = get_matern_kernel_with_gamma_prior(
                    ard_num_dims=train_x.shape[-1], # BoTorch default: 1 length scale per input dim.
                    batch_shape=train_x.shape[:-2],
                )
        model = SingleTaskGP(train_x.detach(),train_y.detach(), covar_module=kernel)
    else:
        model = SingleTaskGP(train_x.detach(),train_y.detach())
    # max_cholesky_size = 4096
    gpytorch.settings.max_cholesky_size(2) #let's not do this.


    
    if torch.get_default_dtype() == torch.float64:
        # print('Casting to double')
        model = model.double()
        model.likelihood = model.likelihood.double()

    mll = ExactMarginalLogLikelihood(model.likelihood, model)
    model = model.to(device)
    mll = mll.to(device)

    try:
        mll = fit_gpytorch_mll(mll)
    except RuntimeError as e:
        print('Fitting GP failed. Retrying. with torch')
        from botorch.optim.fit import fit_gpytorch_mll_torch
        mll = fit_gpytorch_mll(mll, optimizer=fit_gpytorch_mll_torch)


    bounds = test_problem.bounds.to(device)
    

    if config['acq_fn'] == 'random':
        points = get_random_candidates(config, bounds)
    else:
        acq = get_acquisition_function(config['acq_fn'], model, config, model.covar_module.outputscale.item())

        points, value = get_candidates(acq, config, bounds)


    new_x_raw = unnormalize(points, test_problem.bounds)
    new_y_raw = test_problem(new_x_raw).unsqueeze(-1)
    print(f'Got {len(new_x_raw)}  new points.')

    # prevent a memory leak here
    del acq
    model.to('cpu')


    return new_x_raw, new_y_raw, model


        


def run_bo_rounds(config):

    print('Using device:', device)


    test_problem = get_test_problem(config)

    # get start points
    df = pd.read_csv(config['out_dir']+'/experiment_log.csv', index_col=0)
    df = df.loc[df['round'] < config['round_to_rerun']]

    feature_cols = [col for col in df.columns if col.startswith('x')]
    train_x = torch.from_numpy(df[feature_cols].to_numpy()).contiguous()
    train_y = torch.from_numpy(df['y'].to_numpy()).unsqueeze(-1).contiguous()


    experiment_log = []

    new_x, new_y, model = run_one_round(test_problem, train_x, train_y, config)

    for i in range(new_x.shape[0]):
        # make a dict of each point and add to log
        point = {}
        point['round'] = config['round_to_rerun']
        point['y'] = new_y[i].item()
        for j in range(new_x.shape[1]):
            point['x'+str(j)] = new_x[i,j].item()
        point['time'] = time.time()
        experiment_log.append(point)


    # make a dataframe from the log
    # save the dataframe to a csv
    # save the config to a json    
    df_new = pd.DataFrame(experiment_log)
    df = pd.concat([df, df_new], ignore_index=True)
    df.to_csv(os.path.join(config['out_dir'], f'experiment_log_round{config["round_to_rerun"]}_full_exploit.csv'))
    # with open(os.path.join(config['out_dir'], 'config.json'), 'w') as f:
        # json.dump(config, f)




def main():

    config = {
        # 'test_function': 'michaelwicz',
        # 'dim': 10,
        # # 'n_minima': 5,
        # 'seed': 0,
        # # 'width': 0.1,
        # 'n_start_points': 300,
        # 'n_rounds': 20,
        # 'batch_size': 300,
        # 'num_restarts': 10,
        # 'raw_samples': 100,
        # 'acq_fn': 'boss',
        # 'opt': 'torch',
        'out_dir': None,
        # 'keops': False,
        # 'init_min_distance': 0.5, # how much distance any starting point should have to any optimum (unit cube)
        'round_to_rerun': 10,
        'logdet_method': 'svd',
    }
    # TODO added this so i can use sh script to just recompute missing things/rerun metrics
    # without rerunning the whole experiment. Either remove or make nice.
    skip=True

    parser = argparse.ArgumentParser()
    for key, value in config.items():
        if type(value) == bool:
            parser.add_argument(f'--{key}', action='store_true', default=value)
        else:
            parser.add_argument(f'--{key}', type=type(value) if value is not None else None, default=value)
    args = parser.parse_args()


    config_json = json.load(open(os.path.join(args.out_dir, 'config.json'), 'r'))
    del config_json['out_dir']
    del config_json['explore_parameter']

    if skip and os.path.exists(os.path.join(args.out_dir, f'experiment_log_round{args.round_to_rerun}_full_exploit.csv')):
        print('Skipping round', args.round_to_rerun, args.out_dir)
        return

    print(args.round_to_rerun, args.out_dir)

    config.update(vars(args))
    config.update(config_json)

    torch.manual_seed(config['seed'])
    random.seed(config['seed'])

    config['explore_parameter'] = 0.0




    # covar_root_decomposition=False prevents MC sampling from crashing.
    with gpytorch.settings.max_cholesky_size(2), gpytorch.settings.fast_computations(solves=True, covar_root_decomposition=False):
        run_bo_rounds(config)


if __name__ == '__main__':
    main()

